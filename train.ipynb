{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a6/anaconda3/envs/LDS2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import open_clip\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 设置所有 GPU 的随机种子\n",
    "\n",
    "    # 允许 CUDNN 使用非确定性算法以提升性能\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # 42 是任意选择的种子值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置字典，存储可调节的参数\n",
    "CONFIG = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'model_name': 'ViT-B-32',  # 使用的模型名称\n",
    "    'pretrained': 'weights/RemoteCLIP-ViT-B-32.pt',  # 请替换为您的预训练模型路径\n",
    "    'json_file': 'datasets/RSITMD/dataset_RSITMD.json',  # 请替换为您的JSON文件路径\n",
    "    'images_root': 'datasets/RSITMD/images',  # 请替换为您的图像目录路径\n",
    "    'batch_size': 256,  # 根据您的GPU内存进行调整\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 8,\n",
    "    'recall_k_list': [1, 5, 10],\n",
    "    'logs_dir': 'logs',  # 日志文件夹\n",
    "    'warmup_epochs': 0,  # 设置为3，根据需要调整\n",
    "    'min_lr_factor': 0.01,  # 最小学习率因子\n",
    "}\n",
    "# 设置设备\n",
    "device = CONFIG['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 00:39:42,770 INFO: Loaded ViT-B-32 model config.\n"
     ]
    }
   ],
   "source": [
    "# 提取数据集名称\n",
    "dataset_name = os.path.basename(os.path.dirname(CONFIG['json_file']))\n",
    "\n",
    "# 提取预训练权重类型\n",
    "if CONFIG['pretrained']:\n",
    "    pretrained_weights_type = os.path.splitext(os.path.basename(CONFIG['pretrained']))[0]\n",
    "else:\n",
    "    pretrained_weights_type = 'none'\n",
    "\n",
    "# 创建日志目录\n",
    "model_logs_dir = os.path.join(CONFIG['logs_dir'], dataset_name, CONFIG['model_name'], pretrained_weights_type)\n",
    "os.makedirs(model_logs_dir, exist_ok=True)\n",
    "\n",
    "# 配置日志记录\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(model_logs_dir, 'train.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# 创建模型但不加载预训练权重\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    CONFIG['model_name'], pretrained=False)\n",
    "tokenizer = open_clip.get_tokenizer(CONFIG['model_name'])\n",
    "model = model.to(device)\n",
    "\n",
    "# 从本地检查点加载自定义权重\n",
    "if CONFIG['pretrained']:\n",
    "    checkpoint = torch.load(CONFIG['pretrained'], map_location=device)\n",
    "    state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "# 定义训练时的预处理，包括随机数据增强\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.RandomRotation(180),\n",
    "            transforms.RandomRotation(270),\n",
    "        ])\n",
    "    ], p=0.5),\n",
    "    preprocess,  # CLIP 模型的预处理操作\n",
    "])\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, split, transform, tokenizer):\n",
    "        self.json_file = json_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split  # 'train' 或 'test'\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        # 加载JSON文件\n",
    "        with open(self.json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for item in data['images']:\n",
    "            if item['split'] == self.split:\n",
    "                image_file = item['filename']\n",
    "                # 每张图像可能有多条描述\n",
    "                for sentence in item['sentences']:\n",
    "                    self.images.append(image_file)\n",
    "                    self.captions.append(sentence['raw'])\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[idx]\n",
    "        # Tokenizer expects a list of texts\n",
    "        text = self.tokenizer([caption])[0]\n",
    "        return image, text\n",
    "\n",
    "# 定义评估数据集类\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, split, transform):\n",
    "        self.json_file = json_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.split = split  # 'test' 或 'val'\n",
    "        self.images = []\n",
    "        self.captions = []\n",
    "        self.image_ids = []\n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        # 加载JSON文件\n",
    "        with open(self.json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for idx, item in enumerate(data['images']):\n",
    "            if item['split'] == self.split:\n",
    "                image_file = item['filename']\n",
    "                captions = [sentence['raw'] for sentence in item['sentences']]\n",
    "                self.images.append(image_file)\n",
    "                self.captions.append(captions)  # 此图像的描述列表\n",
    "                self.image_ids.append(idx)  # 为每个图像分配唯一ID\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        captions = self.captions[idx]  # 此图像的描述列表\n",
    "        return image, captions, idx  # 返回图像、描述列表和索引\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "batch_size = CONFIG['batch_size']\n",
    "\n",
    "# 训练数据集和加载器\n",
    "train_dataset = CustomDataset(CONFIG['json_file'], CONFIG['images_root'], 'train', train_transform, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "# 验证数据集和加载器\n",
    "val_dataset = EvaluationDataset(CONFIG['json_file'], CONFIG['images_root'], 'test', preprocess)\n",
    "\n",
    "def evaluation_collate_fn(batch):\n",
    "    images, captions_list, indices = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, captions_list, indices\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=CONFIG['num_workers'], pin_memory=True, collate_fn=evaluation_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def evaluate(model, val_loader, tokenizer, device, recall_k_list=[1, 5, 10]):\n",
    "    model.eval()\n",
    "    # 用于存储嵌入和索引的列表\n",
    "    image_embs = []\n",
    "    text_embs = []\n",
    "    texts_image_indices = []\n",
    "    all_captions = []\n",
    "    \n",
    "    # 第一步：计算图像嵌入并收集描述\n",
    "    for batch in tqdm(val_loader, desc=\"Computing image embeddings\"):\n",
    "        images, captions_list, indices = batch\n",
    "        images = images.to(device)\n",
    "        indices = list(indices)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            image_embs.append(image_features)\n",
    "        \n",
    "        # 收集所有描述，并将每个描述映射到其图像索引\n",
    "        for i, captions in zip(indices, captions_list):\n",
    "            all_captions.extend(captions)\n",
    "            texts_image_indices.extend([i] * len(captions))\n",
    "    \n",
    "    # 将图像嵌入连接起来并移动到同一设备\n",
    "    image_embs = torch.cat(image_embs).to(device)\n",
    "    \n",
    "    # 第二步：计算文本嵌入\n",
    "    # 由于可能有很多描述，分批处理\n",
    "    batch_size = 256\n",
    "    num_texts = len(all_captions)\n",
    "    text_embs = []\n",
    "    for i in tqdm(range(0, num_texts, batch_size), desc=\"Computing text embeddings\"):\n",
    "        captions_batch = all_captions[i:i+batch_size]\n",
    "        texts = tokenizer(captions_batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(texts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            text_embs.append(text_features)\n",
    "    text_embs = torch.cat(text_embs).to(device)\n",
    "    \n",
    "    # 计算相似度得分并应用温度缩放\n",
    "    if hasattr(model, 'logit_scale'):\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "    else:\n",
    "        logit_scale = 1.0  # 如果不存在，则默认为 1.0\n",
    "    scores = text_embs @ image_embs.t() * logit_scale  # 形状：(num_texts, num_images)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 图像检索文本\n",
    "    image_ranks = []\n",
    "    for i in range(image_embs.size(0)):\n",
    "        # 获取图像 i 与所有文本的相似度\n",
    "        sims = scores[:, i]\n",
    "        inds = torch.argsort(sims, descending=True)\n",
    "        # 找到与图像 i 相关的所有文本的位置\n",
    "        pos_inds = (torch.tensor(texts_image_indices, device=device) == i).nonzero(as_tuple=False).squeeze()\n",
    "        if pos_inds.ndim == 0:\n",
    "            pos_inds = pos_inds.unsqueeze(0)\n",
    "        # 在所有相关文本中找到最小排名\n",
    "        rank = scores.size(0)  # 初始化排名为最大值\n",
    "        for pos in pos_inds:\n",
    "            pos_rank = (inds == pos).nonzero(as_tuple=False).item()\n",
    "            if pos_rank < rank:\n",
    "                rank = pos_rank\n",
    "        image_ranks.append(rank)\n",
    "    image_ranks = np.array(image_ranks)\n",
    "    \n",
    "    for k in recall_k_list:\n",
    "        r_at_k = 100.0 * np.mean(image_ranks < k)\n",
    "        metrics[f'Retrieval Image to Text R@{k}'] = r_at_k\n",
    "    \n",
    "    # 文本检索图像\n",
    "    text_ranks = []\n",
    "    for i in range(text_embs.size(0)):\n",
    "        # 获取文本 i 与所有图像的相似度\n",
    "        sims = scores[i, :]\n",
    "        inds = torch.argsort(sims, descending=True)\n",
    "        gt_image = texts_image_indices[i]\n",
    "        rank = (inds == gt_image).nonzero(as_tuple=False).item()\n",
    "        text_ranks.append(rank)\n",
    "    text_ranks = np.array(text_ranks)\n",
    "    \n",
    "    for k in recall_k_list:\n",
    "        r_at_k = 100.0 * np.mean(text_ranks < k)\n",
    "        metrics[f'Retrieval Text to Image R@{k}'] = r_at_k\n",
    "    \n",
    "    # 计算平均召回率\n",
    "    if metrics:\n",
    "        recall_values = [metrics[key] for key in metrics.keys()]\n",
    "        metrics['Retrieval Mean Recall'] = np.mean(recall_values)\n",
    "    else:\n",
    "        metrics['Retrieval Mean Recall'] = 0.0\n",
    "        logger.warning(\"No metrics computed during evaluation.\")\n",
    "    \n",
    "    # 打印metrics键以调试\n",
    "    logger.debug(f\"Metrics keys: {list(metrics.keys())}\")\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20:   0%|          | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 00:40:21,438 INFO: Epoch 1/20, Loss: 1.2817, LR: 0.00000994                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.2817\n",
      "Current Learning Rate: 0.00000994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing image embeddings: 0it [00:00, ?it/s]\n",
      "2025-06-04 00:40:21,577 ERROR: Error during evaluation: torch.cat(): expected a non-empty list of Tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: torch.cat(): expected a non-empty list of Tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20:  74%|███████▍  | 62/84 [00:26<00:09,  2.37it/s, lr=0.00000982, loss=0.5543]"
     ]
    }
   ],
   "source": [
    "# 定义对比损失函数，使用模型的 logit_scale 和软标签\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        # 归一化嵌入\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # 计算 logits\n",
    "        logits_per_image = image_embeddings @ text_embeddings.t()\n",
    "        logits_per_text = text_embeddings @ image_embeddings.t()\n",
    "        \n",
    "        # 数值稳定性处理\n",
    "        logits_per_image = logits_per_image - logits_per_image.max(dim=-1, keepdim=True)[0]\n",
    "        logits_per_text = logits_per_text - logits_per_text.max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "        # 从模型中获取 logit_scale 并应用温度缩放\n",
    "        if hasattr(self.model, 'logit_scale'):\n",
    "            logit_scale = self.model.logit_scale.exp()\n",
    "        else:\n",
    "            logit_scale = 1.0  # 如果不存在，则默认为 1.0\n",
    "\n",
    "        logits_per_image = logits_per_image * logit_scale\n",
    "        logits_per_text = logits_per_text * logit_scale\n",
    "\n",
    "        # 计算相似度矩阵（用于软标签）\n",
    "        images_similarity = image_embeddings @ image_embeddings.t()\n",
    "        texts_similarity = text_embeddings @ text_embeddings.t()\n",
    "\n",
    "        # 计算软标签（目标分布）\n",
    "        # 注意，这里也要乘以 logit_scale，以保持尺度一致\n",
    "        targets_per_image = F.softmax((images_similarity + texts_similarity) / 2 * logit_scale, dim=-1)\n",
    "        targets_per_text = targets_per_image.t()  # 转置以获得文本的目标\n",
    "\n",
    "        # 计算图像到文本的损失\n",
    "        loss_i = self.cross_entropy_with_soft_labels(logits_per_image, targets_per_image)\n",
    "        # 计算文本到图像的损失\n",
    "        loss_t = self.cross_entropy_with_soft_labels(logits_per_text, targets_per_text)\n",
    "        # 平均两个损失\n",
    "        loss = (loss_i + loss_t) / 2.0\n",
    "        return loss\n",
    "\n",
    "    def cross_entropy_with_soft_labels(self, logits, soft_targets):\n",
    "        log_prob = F.log_softmax(logits, dim=-1)\n",
    "        loss = (-soft_targets * log_prob).sum(dim=-1).mean()\n",
    "        return loss\n",
    "        \n",
    "# 定义基于步骤的学习率调整函数\n",
    "def lr_lambda_func(step, warmup_steps, total_steps, min_lr_factor=0.0):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))  # 线性 warmup\n",
    "    else:\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine_decay = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return float(min_lr_factor + (1.0 - min_lr_factor) * cosine_decay)             \n",
    "\n",
    "# 初始化自定义损失函数，使用模型的 logit_scale\n",
    "loss_fn = ContrastiveLoss(model).to(device)\n",
    "\n",
    "# 定义需要排除权重衰减的参数关键字\n",
    "no_decay_keywords = ['bias', 'ln_', 'logit_scale']\n",
    "\n",
    "decay_params = []\n",
    "no_decay_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        continue  # 排除冻结的参数\n",
    "    if any(keyword in name for keyword in no_decay_keywords):\n",
    "        no_decay_params.append(param)\n",
    "    else:\n",
    "        decay_params.append(param)\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {'params': decay_params, 'weight_decay': CONFIG['weight_decay']},\n",
    "        {'params': no_decay_params, 'weight_decay': 0.0},\n",
    "    ],\n",
    "    lr=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "# 计算总的训练步骤数和 warmup 步骤数\n",
    "total_steps = CONFIG['num_epochs'] * len(train_loader)\n",
    "warmup_steps = CONFIG['warmup_epochs'] * len(train_loader)\n",
    "\n",
    "# 定义学习率调度器\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: lr_lambda_func(\n",
    "        step,\n",
    "        warmup_steps,\n",
    "        total_steps,\n",
    "        CONFIG['min_lr_factor']\n",
    "    )\n",
    ")\n",
    "\n",
    "num_epochs = CONFIG['num_epochs']\n",
    "best_mean_recall = 0.0\n",
    "global_step = 0  # 全局训练步骤\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        images, texts = batch  # images: [batch_size, 3, H, W], texts: [batch_size, seq_length]\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 获取嵌入\n",
    "        image_embeddings = model.encode_image(images)\n",
    "        text_embeddings = model.encode_text(texts)\n",
    "\n",
    "        # 使用对比损失函数计算损失\n",
    "        loss = loss_fn(image_embeddings, text_embeddings)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 在每个 batch 后更新学习率\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1  # 更新全局训练步骤\n",
    "\n",
    "        # 获取当前学习率\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # 更新进度条后缀，显示当前学习率\n",
    "        progress_bar.set_postfix({'lr': f\"{current_lr:.8f}\", 'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 获取当前学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current Learning Rate: {current_lr:.8f}\")\n",
    "\n",
    "    # 记录学习率到日志\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.8f}\")\n",
    "\n",
    "    # 评估\n",
    "    try:\n",
    "        metrics = evaluate(model, val_loader, tokenizer, device, CONFIG['recall_k_list'])\n",
    "\n",
    "        # 打印所有指标\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "            logger.info(f\"{key}: {value:.2f}\")\n",
    "\n",
    "        # 如果平均召回率提升，保存模型\n",
    "        if 'Retrieval Mean Recall' in metrics:\n",
    "            mean_recall = metrics['Retrieval Mean Recall']\n",
    "            if mean_recall > best_mean_recall:\n",
    "                best_mean_recall = mean_recall\n",
    "                best_model_path = os.path.join(model_logs_dir, f\"{CONFIG['model_name']}_best_model.pt\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"New best model saved with Mean Recall: {best_mean_recall:.2f}\")\n",
    "                logger.info(f\"New best model saved with Mean Recall: {best_mean_recall:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        logger.error(f\"Error during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LDS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
